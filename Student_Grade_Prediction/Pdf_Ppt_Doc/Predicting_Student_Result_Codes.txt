import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
from sklearn.metrics import ConfusionMatrixDisplay

dataset = pd.read_csv('student-mat-pass-or-fail.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#naive
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.naive_bayes import GaussianNB
classifier_naive = GaussianNB()
classifier_naive.fit(X_train, y_train)
y_pred_naive = classifier_naive.predict(X_test)

#logistic
from sklearn.linear_model import LogisticRegression
classifier_logistic = LogisticRegression(random_state = 0)
classifier_logistic.fit(X_train, y_train)
y_pred_logistic = classifier_logistic.decision_function(X_test)

#kernel svm
from sklearn.svm import SVC
classifier_kernel_svm = SVC(kernel = 'rbf', random_state = 0)
classifier_kernel_svm.fit(X_train, y_train)
y_pred_kernel_svm = classifier_kernel_svm.decision_function(X_test)

#svm
from sklearn.svm import SVC
classifier_svm = SVC(kernel = 'linear', random_state = 0)
classifier_svm.fit(X_train, y_train)
y_pred_svm = classifier_svm.decision_function(X_test)

#decision tree
from sklearn.tree import DecisionTreeClassifier
classifier_decision_tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier_decision_tree.fit(X_train, y_train)
y_pred_decision_tree = classifier_decision_tree.predict(X_test)

#random
from sklearn.ensemble import RandomForestClassifier
classifier_random_forest = RandomForestClassifier(n_estimators = 5, criterion = 'entropy', random_state = 0)
classifier_random_forest.fit(X_train, y_train)
y_pred_random_forest = classifier_random_forest.predict(X_test)

#knn
from sklearn.neighbors import KNeighborsClassifier
classifier_knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifier_knn.fit(X_train, y_train)
y_pred_knn = classifier_knn.predict(X_test)

from sklearn.metrics import confusion_matrix, accuracy_score

cm_knn = confusion_matrix(y_test, y_pred_knn)
print(cm_knn)
knn_acc = accuracy_score(y_test, y_pred_knn)

cm_random_forest = confusion_matrix(y_test, y_pred_random_forest)
print(cm_random_forest)
random_forest_acc = accuracy_score(y_test, y_pred_random_forest)

cm_decision_tree = confusion_matrix(y_test, y_pred_decision_tree)
print(cm_decision_tree)
decision_tree_acc = accuracy_score(y_test, y_pred_decision_tree)

cm_svm = confusion_matrix(y_test, y_pred_svm)
print(cm_svm)
svm_acc = accuracy_score(y_test, y_pred_svm)

cm_kernel_svm = confusion_matrix(y_test, y_pred_kernel_svm)
print(cm_svm)
kernel_svm_acc = accuracy_score(y_test, y_pred_kernel_svm)

cm_logistic = confusion_matrix(y_test, y_pred_logistic)
print(cm_logistic)
logistic_acc = accuracy_score(y_test, y_pred_logistic)

cm_naive = confusion_matrix(y_test, y_pred_naive)
print(cm_naive)
naive_acc = accuracy_score(y_test, y_pred_naive)

CM_display_naive = ConfusionMatrixDisplay(cm_naive).plot()
CM_display_knn = ConfusionMatrixDisplay(cm_knn).plot()
CM_display_svm = ConfusionMatrixDisplay(cm_svm).plot()
CM_display_kernel_svm = ConfusionMatrixDisplay(cm_kernel_svm).plot()
CM_display_logistic = ConfusionMatrixDisplay(cm_logistic).plot()
CM_display_dec_tree = ConfusionMatrixDisplay(cm_decision_tree).plot()
CM_display_rand_forest = ConfusionMatrixDisplay(cm_random_forest).plot()

#barplot
plt.figure(figsize=(4,4))
sns.countplot(data = dataset , x = 'pass')

#histogram
plt.figure(figsize=(4,4))
sns.histplot(data = dataset)

dataset.hist(figsize=(30, 20))
plt.show()

#corr heatmap
correlation = dataset.corr()
plt.figure(figsize= (20,16))
sns.heatmap(correlation, cmap= 'coolwarm')

#Roc plot and auc
from sklearn.metrics import roc_curve, auc

naive_fpr, naive_tpr, threshold = roc_curve(y_test, y_pred_naive)
auc_naive = auc(naive_fpr, naive_tpr)

knn_fpr, knn_tpr, threshold = roc_curve(y_test, y_pred_knn)
auc_knn = auc(knn_fpr, knn_tpr)

random_forest_fpr, random_forest_tpr, threshold = roc_curve(y_test, y_pred_random_forest)
auc_random_forest = auc(random_forest_fpr, random_forest_tpr)

kernel_svm_fpr, kernel_svm_tpr, threshold = roc_curve(y_test, y_pred_kernel_svm)
auc_kernel_svm = auc(kernel_svm_fpr, kernel_svm_tpr)

decision_tree_fpr, decision_tree_tpr, threshold = roc_curve(y_test, y_pred_decision_tree)
auc_decision_tree = auc(decision_tree_fpr, decision_tree_tpr)

logistic_fpr, logistic_tpr, threshold = roc_curve(y_test, y_pred_logistic)
auc_logistic = auc(logistic_fpr, logistic_tpr)

svm_fpr, svm_tpr, threshold = roc_curve(y_test, y_pred_svm)
auc_svm = auc(svm_fpr, svm_tpr)

plt.figure(figsize=(5, 5), dpi=100)
plt.plot(naive_fpr, naive_fpr, linestyle='-', label='Naive Bayes (auc = %0.3f)' % auc_naive)
plt.plot(logistic_fpr, logistic_tpr, marker='.', label='Logistic (auc = %0.3f)' % auc_logistic)
plt.plot(svm_fpr, svm_tpr, linestyle='-', label='SVM (auc = %0.3f)' % auc_svm)
plt.plot(kernel_svm_fpr, kernel_svm_tpr, linestyle='-', label='Kernel SVM (auc = %0.3f)' % auc_kernel_svm)
plt.plot(decision_tree_fpr, decision_tree_tpr, linestyle='-', label='Decision Tree (auc = %0.3f)' % auc_decision_tree)
plt.plot(random_forest_fpr, random_forest_tpr, linestyle='-', label='Random Forest(auc = %0.3f)' % auc_random_forest)
plt.plot(knn_fpr, knn_tpr, linestyle='-', label='KNN (auc = %0.3f)' % auc_knn)


plt.xlabel('False Positive Rate -->')
plt.ylabel('True Positive Rate -->')

plt.legend()

plt.show()


#ANN
import numpy as np
import pandas as pd
import tensorflow as tf

dataset = pd.read_csv('student-mat-pass-or-fail.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

print(X)

print(y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

ann = tf.keras.models.Sequential()
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))
ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
ann.fit(X_train, y_train, batch_size = 32, epochs = 100)
y_pred = ann.predict(X_test)
y_pred = (y_pred > 0.5)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

CM_display_ANN = ConfusionMatrixDisplay(cm).plot()